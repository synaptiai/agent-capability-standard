#!/usr/bin/env python3
"""Sync skill-local output schemas from the capability ontology.

Reads schemas/capability_ontology.yaml, extracts each node's output_schema,
and writes a local copy to skills/<name>/schemas/output_schema.yaml.

Also bundles transitive dependencies for workflow catalogs:
- Copies world_state_schema.yaml, event_schema.yaml, and
  transform_mapping_rawlog_to_observation.yaml into reference/ dirs
- Rewrites internal schemas/ paths in bundled workflow_catalog.yaml files

Usage:
    python tools/sync_skill_schemas.py            # generate all schemas
    python tools/sync_skill_schemas.py --dry-run   # preview without writing
"""

from __future__ import annotations

import argparse
import shutil
import sys
from pathlib import Path

import yaml

from _yaml_util import ONTOLOGY_MAX_BYTES, safe_yaml_load

ROOT = Path(__file__).resolve().parents[1]
ONTOLOGY_PATH = ROOT / "schemas" / "capability_ontology.yaml"
SKILLS_DIR = ROOT / "skills"

# Transitive dependencies referenced inside bundled workflow_catalog.yaml.
# Maps repo-root paths → local filenames for both copying and path rewriting.
WORKFLOW_CATALOG_TRANSITIVE_DEPS = {
    "schemas/world_state_schema.yaml": "world_state_schema.yaml",
    "schemas/event_schema.yaml": "event_schema.yaml",
    "schemas/transforms/transform_mapping_rawlog_to_observation.yaml": "transform_mapping_rawlog_to_observation.yaml",
}

# Comment-only references to rewrite with a note
WORKFLOW_CATALOG_COMMENT_REWRITES = {
    "schemas/workflows/": "(repo-level) schemas/workflows/",
}

HEADER_TEMPLATE = """\
# Output schema for the {capability} capability
# Source: schemas/capability_ontology.yaml
# Generated by: tools/sync_skill_schemas.py
# DO NOT EDIT — regenerate with: python tools/sync_skill_schemas.py
"""


def load_ontology() -> dict:
    """Load and return the capability ontology."""
    return safe_yaml_load(ONTOLOGY_PATH, max_size=ONTOLOGY_MAX_BYTES)


def build_layer_index(ontology: dict) -> dict[str, str]:
    """Build a mapping from capability id to layer name."""
    index: dict[str, str] = {}
    for layer_name, layer_info in ontology.get("layers", {}).items():
        for cap_id in layer_info.get("capabilities", []):
            index[cap_id] = layer_name
    return index


def generate_schema_doc(node: dict, layer: str) -> str:
    """Generate the YAML content for a skill's output_schema.yaml."""
    header = HEADER_TEMPLATE.format(capability=node["id"])
    doc = {
        "capability": node["id"],
        "layer": layer,
        "description": node.get("description", ""),
        "output_schema": node.get("output_schema", {}),
    }
    return header + "---\n" + yaml.dump(doc, default_flow_style=False, sort_keys=False)


def sync_schemas(dry_run: bool = False) -> tuple[int, int, list[str]]:
    """Sync output schemas for all ontology nodes.

    Returns (generated_count, skipped_count, errors).
    """
    ontology = load_ontology()
    layer_index = build_layer_index(ontology)
    nodes = ontology.get("nodes", [])

    generated = 0
    skipped = 0
    errors: list[str] = []

    for node in nodes:
        cap_id = node["id"]
        skill_dir = SKILLS_DIR / cap_id

        if not skill_dir.exists():
            errors.append(f"Skill directory missing: skills/{cap_id}/")
            skipped += 1
            continue

        layer = layer_index.get(cap_id, "UNKNOWN")
        content = generate_schema_doc(node, layer)
        schema_dir = skill_dir / "schemas"
        schema_path = schema_dir / "output_schema.yaml"

        if dry_run:
            print(f"  [dry-run] Would write: {schema_path.relative_to(ROOT)}")
            generated += 1
            continue

        schema_dir.mkdir(parents=True, exist_ok=True)
        schema_path.write_text(content, encoding="utf-8")
        generated += 1

    return generated, skipped, errors


def sync_workflow_catalog_deps(dry_run: bool = False) -> tuple[int, int, list[str]]:
    """Bundle transitive dependencies for skills with reference/workflow_catalog.yaml.

    For each skill that has a bundled workflow_catalog.yaml in its reference/ dir,
    this function:
    1. Copies transitive schema dependencies into the same reference/ dir
    2. Rewrites internal schemas/ paths in the workflow_catalog.yaml to use
       co-located relative filenames

    Returns (bundled_count, rewritten_count, errors).
    """
    bundled = 0
    rewritten = 0
    errors: list[str] = []

    catalog_files = sorted(SKILLS_DIR.glob("*/reference/workflow_catalog.yaml"))
    if not catalog_files:
        return 0, 0, []

    # Step 1: Copy transitive dependencies into each reference/ dir
    for catalog_path in catalog_files:
        ref_dir = catalog_path.parent
        skill_name = ref_dir.parent.name

        for repo_path, local_name in WORKFLOW_CATALOG_TRANSITIVE_DEPS.items():
            src = ROOT / repo_path
            dst = ref_dir / local_name

            if not src.exists():
                errors.append(
                    f"[{skill_name}] Transitive dep missing at repo root: {repo_path}"
                )
                continue

            if dry_run:
                print(f"  [dry-run] Would copy: {repo_path} → skills/{skill_name}/reference/{local_name}")
            else:
                shutil.copy2(src, dst)
            bundled += 1

    # Step 2: Rewrite schemas/ paths inside each bundled workflow_catalog.yaml
    for catalog_path in catalog_files:
        skill_name = catalog_path.parent.parent.name
        content = catalog_path.read_text(encoding="utf-8")
        original = content

        # Rewrite structural refs (ref:, mapping_ref:, output_conforms_to:)
        for old_path, new_path in WORKFLOW_CATALOG_TRANSITIVE_DEPS.items():
            content = content.replace(old_path, new_path)

        # Rewrite comment-only domain workflow refs
        for old_prefix, new_prefix in WORKFLOW_CATALOG_COMMENT_REWRITES.items():
            content = content.replace(old_prefix, new_prefix)

        if content != original:
            if dry_run:
                print(f"  [dry-run] Would rewrite paths in: skills/{skill_name}/reference/workflow_catalog.yaml")
            else:
                catalog_path.write_text(content, encoding="utf-8")
            rewritten += 1

    return bundled, rewritten, errors


def main() -> None:
    ap = argparse.ArgumentParser(
        description="Sync skill-local output schemas from capability ontology"
    )
    ap.add_argument(
        "--dry-run",
        action="store_true",
        help="Preview what would be generated without writing files",
    )
    args = ap.parse_args()

    if not ONTOLOGY_PATH.exists():
        print(f"ERROR: Ontology not found: {ONTOLOGY_PATH}")
        sys.exit(1)

    # Phase 1: Sync output schemas
    generated, skipped, errors = sync_schemas(dry_run=args.dry_run)

    if errors:
        print("WARNINGS (schemas):")
        for e in errors:
            print(f"  - {e}")

    mode = "[dry-run] " if args.dry_run else ""
    print(f"\n{mode}SCHEMA SYNC: {generated} schemas generated, {skipped} skipped")

    # Phase 2: Bundle transitive deps for workflow catalogs
    bundled, rewritten, wf_errors = sync_workflow_catalog_deps(dry_run=args.dry_run)
    errors.extend(wf_errors)

    if wf_errors:
        print("WARNINGS (workflow catalog deps):")
        for e in wf_errors:
            print(f"  - {e}")

    print(f"{mode}WORKFLOW CATALOG DEPS: {bundled} files bundled, {rewritten} catalogs rewritten")
    print(f"\n{mode}SYNC COMPLETE")

    if errors:
        sys.exit(1)


if __name__ == "__main__":
    main()
